{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import facebook_scraper as fs\n",
    "import lxml.html.clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in fs.get_posts('facebook', pages=100):\n",
    "     print(post['text'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'comments_full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# extract the comments part\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m comments \u001b[38;5;241m=\u001b[39m \u001b[43mpost\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomments_full\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# process comments as you want...\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m comments:\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# e.g. ...print them\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'comments_full'"
     ]
    }
   ],
   "source": [
    "POST_ID = \"pfbid02FoUuC6NYMheGK27AYe5pF9cwod4EFQM9MYRxenjMFy7MhkKTxDn5KFiR6D4X7d4pl\"\n",
    "\n",
    "# number of comments to download -- set this to True to download all comments\n",
    "MAX_COMMENTS = 100\n",
    "\n",
    "# get the post (this gives a generator)\n",
    "gen = fs.get_posts(\n",
    "    post_urls=[POST_ID],\n",
    "    options={\"comments\": MAX_COMMENTS, \"progress\": True}\n",
    ")\n",
    "\n",
    "# take 1st element of the generator which is the post we requested\n",
    "post = next(gen)\n",
    "\n",
    "# extract the comments part\n",
    "comments = post['comments_full']\n",
    "\n",
    "# process comments as you want...\n",
    "for comment in comments:\n",
    "\n",
    "    # e.g. ...print them\n",
    "    print(comment)\n",
    "\n",
    "    # e.g. ...get the replies for them\n",
    "    for reply in comment['replies']:\n",
    "        print(' ', reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: facebook-scraper [-h] [-f FILENAME] [-p PAGES] [-s SLEEP] [-t TIMEOUT]\n",
      "                        [-g] [-v] [-c COOKIES] [--comments] [-r] [-rs]\n",
      "                        [--dump DUMP_LOCATION] [--encoding ENCODING]\n",
      "                        [-fmt {csv,json}] [-d DAYS_LIMIT] [-rf RESUME_FILE]\n",
      "                        [-ner] [-k KEYS] [-m MATCHING] [-nm NOT_MATCHING]\n",
      "                        [--extra-info] [--use-youtube-dl] [--profile]\n",
      "                        [--friends FRIENDS] [-ppp POSTS_PER_PAGE] [--source]\n",
      "                        account\n",
      "\n",
      "Scrape Facebook public pages without an API key\n",
      "\n",
      "positional arguments:\n",
      "  account               Facebook account or group\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -f FILENAME, --filename FILENAME\n",
      "                        Output filename\n",
      "  -p PAGES, --pages PAGES\n",
      "                        Number of pages to download\n",
      "  -s SLEEP, --sleep SLEEP\n",
      "                        How long to sleep for between posts\n",
      "  -t TIMEOUT, --timeout TIMEOUT\n",
      "                        How long to wait in seconds for Facebook servers\n",
      "                        before aborting\n",
      "  -g, --group           Use group scraper\n",
      "  -v, --verbose         Enable logging\n",
      "  -c COOKIES, --cookies COOKIES\n",
      "                        Path to a cookies file\n",
      "  --comments            Extract comments\n",
      "  -r, --reactions       Extract reactions\n",
      "  -rs, --reactors       Extract reactors\n",
      "  --dump DUMP_LOCATION  Location where to save the HTML source of the posts\n",
      "                        (useful for debugging)\n",
      "  --encoding ENCODING   Encoding for the output file\n",
      "  -fmt {csv,json}, --format {csv,json}\n",
      "                        What format to export as\n",
      "  -d DAYS_LIMIT, --days-limit DAYS_LIMIT\n",
      "                        Number of days to download\n",
      "  -rf RESUME_FILE, --resume-file RESUME_FILE\n",
      "                        Filename to store the last pagination URL in, for\n",
      "                        resuming\n",
      "  -ner, --no-extra-requests\n",
      "                        Disable making extra requests (for things like high\n",
      "                        quality image URLs)\n",
      "  -k KEYS, --keys KEYS  Comma separated list of which keys or columns to\n",
      "                        return. This lets you filter to just your desired\n",
      "                        outputs.\n",
      "  -m MATCHING, --matching MATCHING\n",
      "                        Filter to just posts matching regex expression\n",
      "  -nm NOT_MATCHING, --not-matching NOT_MATCHING\n",
      "                        Filter to just posts not matching regex expression\n",
      "  --extra-info          Try to do an extra request to get the post reactions.\n",
      "                        Default is False\n",
      "  --use-youtube-dl      Use Youtube-DL for (high-quality) video extraction.\n",
      "                        You need to have youtube-dl installed on your\n",
      "                        environment. Default is False.\n",
      "  --profile             Extract an account's profile\n",
      "  --friends FRIENDS     When extracting a profile, how many friends to extract\n",
      "  -ppp POSTS_PER_PAGE, --posts-per-page POSTS_PER_PAGE\n",
      "                        Number of posts to fetch per page\n",
      "  --source              Include HTML source\n"
     ]
    }
   ],
   "source": [
    "!facebook-scraper --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't get any posts.\n"
     ]
    }
   ],
   "source": [
    "!facebook-scraper --filename nintendo_page_posts.csv --pages 10 nintendo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding using snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape as sns\n",
    "import lxml.html.clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: snscrape [-h] [--version] [--citation] [-v] [--dump-locals] [--retry N]\n",
      "                [-n N] [-f FORMAT | --jsonl | --jsonl-for-buggy-int-parser]\n",
      "                [--with-entity] [--since DATETIME] [--progress]\n",
      "                SCRAPER ...\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --version             show program's version number and exit\n",
      "  --citation            Display recommended citation information and exit\n",
      "                        (default: None)\n",
      "  -v, --verbose, --verbosity\n",
      "                        Increase output verbosity (default: 0)\n",
      "  --dump-locals         Dump local variables on serious log messages (warnings\n",
      "                        or higher) (default: False)\n",
      "  --retry N, --retries N\n",
      "                        When the connection fails or the server returns an\n",
      "                        unexpected response, retry up to N times with an\n",
      "                        exponential backoff (default: 3)\n",
      "  -n N, --max-results N\n",
      "                        Only return the first N results (default: None)\n",
      "  -f FORMAT, --format FORMAT\n",
      "                        Output format (default: None)\n",
      "  --jsonl               Output JSONL (default: False)\n",
      "  --jsonl-for-buggy-int-parser\n",
      "                        Output JSONL and insert extra string fields into\n",
      "                        objects for integers exceeding double precision limits\n",
      "                        (default: False)\n",
      "  --with-entity         Include the entity (e.g. user, channel) as the first\n",
      "                        output item (default: False)\n",
      "  --since DATETIME      Only return results newer than DATETIME (default:\n",
      "                        None)\n",
      "  --progress            Report progress on stderr (default: False)\n",
      "\n",
      "scrapers:\n",
      "  SCRAPER\n",
      "    facebook-community\n",
      "    facebook-group\n",
      "    facebook-user\n",
      "    instagram-hashtag\n",
      "    instagram-location\n",
      "    instagram-user\n",
      "    mastodon-profile\n",
      "    mastodon-toot\n",
      "    reddit-search\n",
      "    reddit-submission\n",
      "    reddit-subreddit\n",
      "    reddit-user\n",
      "    telegram-channel\n",
      "    twitter-cashtag\n",
      "    twitter-community\n",
      "    twitter-hashtag\n",
      "    twitter-list-posts\n",
      "    twitter-profile\n",
      "    twitter-search\n",
      "    twitter-trends\n",
      "    twitter-tweet\n",
      "    twitter-user\n",
      "    twitter-users\n",
      "    vkontakte-user\n",
      "    weibo-user\n"
     ]
    }
   ],
   "source": [
    "!snscrape --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-16 15:51:04.502  ERROR  snscrape.base  Error retrieving https://twitter.com/textfile: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /textfile (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\"))\n",
      "2025-03-16 15:51:04.503  CRITICAL  snscrape.base  4 requests to https://twitter.com/textfile failed, giving up.\n",
      "2025-03-16 15:51:04.503  CRITICAL  snscrape.base  Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /textfile (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /textfile (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /textfile (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /textfile (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)')))\"))\n",
      "2025-03-16 15:51:04.525  CRITICAL  snscrape._cli  Dumped stack and locals to /var/folders/5j/g30ypjc12sv03pdwnk568clm0000gn/T/snscrape_locals_xpnpsos6\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/bin/snscrape\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/_cli.py\", line 323, in main\n",
      "    for i, item in enumerate(scraper.get_items(), start = 1):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/modules/twitter.py\", line 1824, in get_items\n",
      "    yield from super().get_items()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/modules/twitter.py\", line 1763, in get_items\n",
      "    for obj in self._iter_api_data('https://twitter.com/i/api/graphql/7jT5GT59P8IFjgxwqnEdQw/SearchTimeline', _TwitterAPIType.GRAPHQL, params, paginationParams, cursor = self._cursor, instructionsPath = ['data', 'search_by_raw_query', 'search_timeline', 'timeline', 'instructions']):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/modules/twitter.py\", line 915, in _iter_api_data\n",
      "    obj = self._get_api_data(endpoint, apiType, reqParams, instructionsPath = instructionsPath)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/modules/twitter.py\", line 883, in _get_api_data\n",
      "    self._ensure_guest_token()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/modules/twitter.py\", line 825, in _ensure_guest_token\n",
      "    r = self._get(self._baseUrl if url is None else url, responseOkCallback = self._check_guest_token_response)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/base.py\", line 275, in _get\n",
      "    return self._request('GET', *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/snscrape/base.py\", line 271, in _request\n",
      "    raise ScraperException(msg)\n",
      "snscrape.base.ScraperException: 4 requests to https://twitter.com/textfile failed, giving up.\n"
     ]
    }
   ],
   "source": [
    "!snscrape twitter-user textfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
